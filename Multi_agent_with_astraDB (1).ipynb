{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBpNNZPWpaZV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install langchain  astrapy groq langchain_huggingface langchain_groq langchain_text_splitters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U sentence-transformers transformers"
      ],
      "metadata": {
        "id": "S3f9qFT9_JJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "ASTRA_DB_TOKEN=userdata.get('ASTRA_DB_TOKEN')\n",
        "ASTRA_DB_ID=userdata.get('ASTRA_DB_ID')\n",
        "groq_key=userdata.get('GROQ_API_KEY')\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "SERPER_API_KEY=userdata.get('SERPER_API_KEY')\n",
        "ASTRA_DB_ENDPOINT=\"https://7c36ff6b-70bc-4a9b-8e2d-2a8aecc92ac1-us-east1.apps.astra.datastax.com\""
      ],
      "metadata": {
        "id": "m1bnWNfR6i86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-community arxiv"
      ],
      "metadata": {
        "id": "pvdaICnzPboo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_community.utilities import ArxivAPIWrapper\n",
        "from typing import List, Dict\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "from astrapy import DataAPIClient\n",
        "import logging\n",
        "logging.getLogger(\"astrapy\").setLevel(logging.ERROR)\n"
      ],
      "metadata": {
        "id": "Xkh82U3J6r4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Researchtools:\n",
        "  def __init__(self,serper_api_key:str):\n",
        "    self.arxiv=ArxivAPIWrapper(\n",
        "        top_k_results=5,\n",
        "        ARXIV_MAX_QUERY_LENGTH=400,\n",
        "        load_max_documents=4,\n",
        "        load_all_available_meta=True)\n",
        "\n",
        "    self.serper_key=serper_api_key\n",
        "  def search_arxiv(self,query:str):\n",
        "    print(f\"Arxiv search:\")\n",
        "    try:\n",
        "      result=self.arxiv.run(query)\n",
        "      return[{\n",
        "          \"type\":\"research_paper\",\n",
        "          \"content\":result,\n",
        "          \"source\":\"arxiv\",\n",
        "          \"query\":query\n",
        "      }]\n",
        "    except Exception as e:\n",
        "      print(f\"Arxiv search failed:m\")\n",
        "\n",
        "    return []\n",
        "  def search_job(self,query:str,max_results:int=5)->List[Dict]:\n",
        "    print(f\"fetching url{query}\")\n",
        "\n",
        "    url=\"https://google.serper.dev/search\"\n",
        "\n",
        "    payload={\n",
        "        \"q\":query,\n",
        "        \"num\":max_results,\n",
        "        \"location\":\"India\"\n",
        "    }\n",
        "    headers={\n",
        "        'X-API-KEY':self.serper_key,\n",
        "        'Content_type':'application/json'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      response=requests.post(url,\n",
        "                            json=payload,\n",
        "                              headers=headers,\n",
        "                              )\n",
        "      response.raise_for_status()\n",
        "      data=response.json()\n",
        "      jobs=data.get('jobs',[])\n",
        "\n",
        "      formatted=[]\n",
        "      for job in jobs:\n",
        "        formatted.append({\n",
        "              \"type\": \"job_posting\",\n",
        "              \"title\": job.get('title', 'N/A'),\n",
        "              \"company\": job.get('company', 'N/A'),\n",
        "              \"location\": job.get('location', 'N/A'),\n",
        "              \"description\": job.get('description', 'N/A'),\n",
        "              \"source_url\": job.get('link', 'N/A'),\n",
        "              \"posted_at\": job.get('posted', 'N/A'),\n",
        "              \"via\": job.get('via', 'N/A')\n",
        "\n",
        "\n",
        "          })\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "              print(\"Error fetching it\")\n",
        "              return []\n",
        "\n",
        "  def execute_tools(self,tool_name:str,query:str,**kwargs)->List[Dict]:\n",
        "    if tool_name==\"arxiv\":\n",
        "      return self.search_arxiv(query)\n",
        "    elif tool_name==\"job_search\":\n",
        "      max_results=kwargs.get('max_results',5)\n",
        "      return self.search_job(query,max_results)\n",
        "    elif tool_name==\"fetching_url\":\n",
        "      content=self.fetching_url(query,kwargs.get('max_results',5000))\n",
        "      if content:\n",
        "           return [{\n",
        "                      \"type\": \"webpage\",\n",
        "                      \"url\": query,\n",
        "                      \"content\": content\n",
        "                  }]\n",
        "           return []\n",
        "      else:\n",
        "          print(f\"error fetching{tool_name}\")\n",
        "          return []\n",
        "  def fetching_url(self,url:str,max_cahrs:int=5000):\n",
        "    print(\"Fetching url\")\n",
        "\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "      print(\"invalid  url format\")\n",
        "      return None\n",
        "    try:\n",
        "      jina_url=f\"https://r.jina.ai/{url}\"\n",
        "      response=requests.get(jina_url)\n",
        "      response.raise_for_status()\n",
        "      content=response.text\n",
        "      print(\"fetched\")\n",
        "      return content\n",
        "\n",
        "\n",
        "    except:\n",
        "      print(\"error jina\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g0dQ0Cz-FGcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlannerAgent:\n",
        "  def __init__(self,groq_key:str):\n",
        "    self.llm=ChatGroq(api_key=groq_key, model=\"llama-3.1-8b-instant\")\n",
        "\n",
        "\n",
        "  def create_plan(self,user_goal:str):\n",
        "        prompt = f\"\"\"You are a research planning assistant. Break down this goal into 3-5 specific, actionable research tasks.\n",
        "\n",
        "        User Goal: {user_goal}\n",
        "\n",
        "        For each task, specify:\n",
        "        - task_id: number (1, 2, 3, etc.)\n",
        "        - description: specific research action\n",
        "        - tool: which tool to use\n",
        "          - \"arxiv\" for academic papers/research\n",
        "          - \"job_search\" for internships/jobs\n",
        "          - \"web_fetch\" for specific URLs\n",
        "\n",
        "        Return ONLY a JSON array:\n",
        "          [\n",
        "            {{\"task_id\": 1, \"description\": \"Search for papers on...\", \"tool\": \"arxiv\"}},\n",
        "            {{\"task_id\": 2, \"description\": \"Find internships at...\", \"tool\": \"job_search\"}}\n",
        "          ]\n",
        "\n",
        "          Return ONLY valid JSON, no 'explanation.\"\"\"\n",
        "\n",
        "        response = self.llm.client.create(\n",
        "                      model=\"llama-3.1-8b-instant\",\n",
        "                      messages=[\n",
        "                          {\"role\": \"system\", \"content\": \"You are a research planner. Always output valid JSON.\"},\n",
        "                          {\"role\": \"user\", \"content\": prompt}\n",
        "                      ],\n",
        "                      temperature=0.1,\n",
        "                      max_tokens=1500,\n",
        "                      response_format={\"type\": \"json_object\"}\n",
        "                  )\n",
        "\n",
        "\n",
        "        raw_content = response.choices[0].message.content\n",
        "        data = json.loads(raw_content)\n",
        "        return data.get(\"tasks\", [])"
      ],
      "metadata": {
        "id": "2bzD5Y8iSXOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "\n",
        "class ResearcherAgent:\n",
        "    def __init__(self, groq_key: str, astra_token: str, astra_endpoint: str,serper_api_key):\n",
        "\n",
        "        self.llm = ChatGroq(api_key=groq_key, model=\"llama-3.1-8b-instant\")\n",
        "        self.embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        self.serper_api_key = serper_api_key\n",
        "\n",
        "        self.arxiv_client = arxiv.Client()\n",
        "\n",
        "\n",
        "        client = DataAPIClient(token=astra_token)\n",
        "        db = client.get_database(astra_endpoint)\n",
        "        self.collection = db.get_collection(\"ResearchPapers\")\n",
        "    def search_arxiv(self, query):\n",
        "      if not getattr(self, 'arxiv_enabled', True):\n",
        "        return []\n",
        "\n",
        "    def execute_task(self, task: Dict) -> List[Dict]:\n",
        "        query = task.get('description', '')\n",
        "        print(f\"Searching Arxiv for: {query}\")\n",
        "\n",
        "\n",
        "        search = arxiv.Search(\n",
        "            query=query,\n",
        "            max_results=10,\n",
        "            sort_by=arxiv.SortCriterion.Relevance\n",
        "        )\n",
        "\n",
        "        processed_results = []\n",
        "        for r in self.arxiv_client.results(search):\n",
        "            # Map Arxiv metadata to our structure\n",
        "            paper = {\n",
        "                \"title\": r.title,\n",
        "                \"authors\": [a.name for a in r.authors],\n",
        "                \"link\": r.entry_id,\n",
        "                \"pdf_url\":r.pdf_url,\n",
        "                \"category\":r.primary_category,\n",
        "                \"summary\": r.summary[:500].replace('\\n', ' ') + \"...\", # Truncate for the table\n",
        "                \"published\": r.published.strftime(\"%Y-%m-%d\"),\n",
        "                \"researched_at\": datetime.now().isoformat()\n",
        "            }\n",
        "            processed_results.append(paper)\n",
        "\n",
        "\n",
        "        if processed_results:\n",
        "            self.store_findings(processed_results)\n",
        "\n",
        "        return processed_results\n",
        "\n",
        "    def store_findings(self, findings: List[Dict]):\n",
        "        if not findings: return\n",
        "\n",
        "        for f in findings:\n",
        "\n",
        "            doc_id = hashlib.md5(f['link'].encode()).hexdigest()[:16]\n",
        "            vector = self.embedder.embed_query(f\"{f['title']} {f['summary']}\")\n",
        "\n",
        "            doc_to_store = {\"_id\": doc_id, \"$vector\": vector, \"metadata\": f}\n",
        "\n",
        "            try:\n",
        "                self.collection.replace_one(filter={\"_id\": doc_id}, replacement=doc_to_store, upsert=True)\n",
        "            except Exception as e:\n",
        "                # If the collection is missing, this will tell us exactly why once\n",
        "                if \"COLLECTION_NOT_EXIST\" in str(e):\n",
        "                    print(\"You need to run the 'create_collection' cell!\")\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"Error storing finding {doc_id}: {e}\")\n",
        "                continue"
      ],
      "metadata": {
        "id": "kPa5RihX6rw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from astrapy.info import CollectionDefinition\n",
        "from astrapy.constants import VectorMetric\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "\n",
        "    definition = (\n",
        "        CollectionDefinition.builder()\n",
        "        .set_vector_dimension(384)\n",
        "        .set_vector_metric(VectorMetric.COSINE)\n",
        "        .build()\n",
        "    )\n",
        "\n",
        "    db.create_collection(\"ResearchPapers\", definition=definition)\n",
        "    print(\"Collection 'ResearchPapers' created successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Status: {e}\")"
      ],
      "metadata": {
        "id": "vkr20z-cHyXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CriticAgent:\n",
        "  def __init__(self,groq_key:str):\n",
        "    self.llm=ChatGroq(api_key=groq_key, model=\"llama-3.1-8b-instant\")\n",
        "  def validate_findings(self,findings:List[Dict])->List[Dict]:\n",
        "    if not findings:\n",
        "      return {\"total_findings\":0,\n",
        "              \"confidence_score\":0.0,\n",
        "              \"issue_found\":[\"no data provided by the researcher.\"],\n",
        "              \"recommendatiosn\":[\"check tool API keys or try a broader search query.\"]}\n",
        "    sample_for_llm=[\n",
        "        {\n",
        "            \"title\": f.get('title',f.get('company','Unknon')),\n",
        "            \"has_link\":bool(f.get('source_url') or f.get('link') or f.get('url')),\n",
        "            \"has_text\":len(f.get('description',f.get('content','')))>20\n",
        "        }\n",
        "        for f in findings[:10]\n",
        "\n",
        "    ]\n",
        "    prompt = f\"\"\"You are a Research Auditor.\n",
        "        I have found {len(findings)} results. Here is a sample of the first 10:\n",
        "\n",
        "        {json.dumps(sample_for_llm, indent=2)}\n",
        "\n",
        "        Analyze if these results are high quality and relevant.\n",
        "        Return a JSON object with these exact keys:\n",
        "        - total_findings (integer)\n",
        "        - issues_found (list of strings)\n",
        "        - confidence_score (float between 0.0 and 1.0)\n",
        "        - recommendations (list of strings)\n",
        "        \"\"\"\n",
        "\n",
        "    try:\n",
        "            # 4. Call Groq with JSON Mode enabled\n",
        "            response = self.llm.client.create(\n",
        "                model=\"llama-3.1-8b-instant\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a quality control AI. Always output valid JSON.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "                temperature=0.1  # Low temperature for consistent \"grading\"\n",
        "            )\n",
        "\n",
        "            # 5. Parse the direct JSON output (No cleaning needed!)\n",
        "            return json.loads(response.choices[0].message.content)\n",
        "\n",
        "    except Exception as e:\n",
        "            print(f\"Critic Analysis Error: {e}\")\n",
        "            return {\n",
        "                \"total_findings\": len(findings),\n",
        "                \"confidence_score\": 0.5,\n",
        "                \"issues_found\": [\"Error during automated critique\"],\n",
        "                \"recommendations\": [\"Manual verification required\"]\n",
        "            }"
      ],
      "metadata": {
        "id": "c9DrPtSgW3uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResearchOrchestrator:\n",
        "  def __init__(self,PlannerAgent,ResearcherAgent,CriticAgent):\n",
        "    self.planner=PlannerAgent\n",
        "    self.researcher=ResearcherAgent\n",
        "    self.critic=CriticAgent\n",
        "  def run(self, user_goal: str, max_iterations: int = 2):\n",
        "    plan=self.planner.create_plan(user_goal)\n",
        "    all_final_findings = []\n",
        "    for i in range(max_iterations):\n",
        "      current_findings=[]\n",
        "      for task in plan:\n",
        "        findings=self.researcher.execute_task(task)\n",
        "        current_findings.extend(findings)\n",
        "        all_final_findings.extend(current_findings)\n",
        "\n",
        "      report=self.critic.validate_findings(all_final_findings)\n",
        "      total_findings=report.get('total_findings',0)\n",
        "      confidence=report.get('confidence_score',0.0)\n",
        "\n",
        "      if confidence>0.8:\n",
        "        self.researcher.store_findings(current_findings)\n",
        "        return current_findings\n",
        "      else:\n",
        "        print(f\"Issue:{report.get('issues_found',[])}\")\n",
        "\n",
        "        self.researcher.store_findings(all_final_findings)\n",
        "        return all_final_findings\n"
      ],
      "metadata": {
        "id": "_tRho4EhW3rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # 1. Initialize all Agents\n",
        "  planner = PlannerAgent(groq_key=groq_key)\n",
        "\n",
        "  # REMOVED serper_api_key here\n",
        "  researcher = ResearcherAgent(\n",
        "      groq_key=groq_key,\n",
        "      astra_token=ASTRA_DB_TOKEN,\n",
        "      astra_endpoint=ASTRA_DB_ENDPOINT,\n",
        "      serper_api_key=SERPER_API_KEY # Added the missing argument\n",
        "  )\n",
        "\n",
        "  # Use 'groq_key' to match your other agents\n",
        "  critic = CriticAgent(groq_key=groq_key)\n",
        "\n",
        "  # 2. Setup the Orchestrator\n",
        "  boss = ResearchOrchestrator(planner, researcher, critic)\n",
        "\n",
        "  boss.run(\"Find 2026 AI internship roles in San Francisco\")"
      ],
      "metadata": {
        "id": "kQEiF_QDW3os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OmzRQ7xj30oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnRxKOOB30gJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}